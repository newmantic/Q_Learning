{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4Lv/PQN+IjAHModVeiN5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newmantic/Q_Learning/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sT7-fMc_u1x_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = np.zeros((n_states, n_actions))  # Initialize Q-table with zeros\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(self.n_actions))  # Explore: random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploit: action with max Q-value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error\n",
        "\n",
        "    def get_q_table(self):\n",
        "        return self.q_table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment:\n",
        "    def __init__(self, grid_size=(5, 5), start_state=(0, 0), goal_state=(4, 4)):\n",
        "        self.grid_size = grid_size\n",
        "        self.start_state = start_state\n",
        "        self.goal_state = goal_state\n",
        "        self.state = start_state\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self._get_state_index(self.state)\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        if action == 0:  # up\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 1:  # down\n",
        "            x = min(self.grid_size[0] - 1, x + 1)\n",
        "        elif action == 2:  # left\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 3:  # right\n",
        "            y = min(self.grid_size[1] - 1, y + 1)\n",
        "\n",
        "        self.state = (x, y)\n",
        "        reward = 1 if self.state == self.goal_state else -1\n",
        "        done = self.state == self.goal_state\n",
        "        return self._get_state_index(self.state), reward, done\n",
        "\n",
        "    def _get_state_index(self, state):\n",
        "        return state[0] * self.grid_size[1] + state[1]"
      ],
      "metadata": {
        "id": "eydTpAYPv5R3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_q_learning_agent():\n",
        "    env = GridEnvironment()\n",
        "    n_states = env.grid_size[0] * env.grid_size[1]\n",
        "    n_actions = 4  # up, down, left, right\n",
        "    agent = QLearningAgent(n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "    n_episodes = 1000\n",
        "    max_steps = 100\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    return agent\n",
        "\n",
        "# Train the agent and print the Q-table\n",
        "q_agent = train_q_learning_agent()\n",
        "print(\"Q-Table after training:\")\n",
        "print(q_agent.get_q_table())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMIM_MJqv6eC",
        "outputId": "d4118be1-a777-42b5-e996-89ced45d501d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table after training:\n",
            "[[-6.65911812 -5.86127292 -6.65292129 -5.86126876]\n",
            " [-5.5848814  -4.91040888 -6.17007447 -4.9104042 ]\n",
            " [-4.35109491 -3.94993745 -4.59279438 -3.94993862]\n",
            " [-3.27692686 -2.97977144 -3.42833482 -2.97976824]\n",
            " [-2.24416921 -1.99979915 -2.53769354 -2.25008515]\n",
            " [-5.92644641 -4.91041623 -5.3518503  -4.91041329]\n",
            " [-4.92473888 -3.94998548 -4.51099641 -3.94998416]\n",
            " [-3.96694546 -2.97979774 -3.83900484 -2.97979793]\n",
            " [-2.79360396 -1.99980037 -2.75761127 -1.99980042]\n",
            " [-1.98385788 -1.0099     -2.14265483 -1.68590391]\n",
            " [-4.77181011 -3.94995835 -4.37068715 -3.94995507]\n",
            " [-4.05950036 -2.97979977 -3.75109316 -2.97979942]\n",
            " [-3.20213917 -1.99980071 -2.47652596 -1.99980068]\n",
            " [-2.26478406 -1.00989999 -1.87139936 -1.00989999]\n",
            " [-1.16024816 -0.01       -1.3233302  -0.71928029]\n",
            " [-3.75638273 -2.97985191 -3.41157967 -2.97979651]\n",
            " [-2.76582827 -1.99997796 -3.04961108 -1.999801  ]\n",
            " [-1.94791025 -1.01251205 -2.15638366 -1.0099    ]\n",
            " [-1.34099146 -0.08843837 -1.5807369  -0.01      ]\n",
            " [-0.9518665   1.         -0.98137287 -0.00995874]\n",
            " [-2.41606034 -2.15785445 -2.07394687 -2.00056952]\n",
            " [-1.69083287 -1.33032495 -1.53307856 -1.01010345]\n",
            " [-1.35458169 -0.56836052 -1.08661585 -0.0100106 ]\n",
            " [-0.34658523 -0.07811238 -0.28651666  0.99999956]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVWeta3Gv89p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}